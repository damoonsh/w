---
title: "Hierarchical Reasoning Model"
date: 2025-08-02
---

Paper: https://www.alphaxiv.org/abs/2506.21734 


# Hierarchical Reasoning Model

# Context


Thinking Fast, Thking Slow:
* Lots of research papers refer to this book
* The book talks divides brain's thinking process into slow and fast
    * Fast: Fast, Automatic, Intuitive
    * Slow: Deloberate, effortful reasoning

<figure style="text-align: center;">
      <img src='https://github.com/damoonsh/DeepDream-Exploration/blob/main/gifs/IM_2_W1_S.gif?raw=true' style='width: auto; height: 30%; '/>
      <figcaption>DeepDream iterations </figcaption>
    </figure>


# Introduction

Previous reasoning models use CoT (Chain-of-Thought), downsides:
- rittle task decomposition
- extensive data requirements
- high latency

# Method

# Result

### üìä ARC-AGI Performance Comparison (HRM vs. Baselines)

| Model | Size (Params) | ARC-AGI-1 (%) | ARC-AGI-2 (%) | Pretraining? | CoT Used? | Notes |
|-------|---------------|---------------|---------------|-------------|----------|-------|
| **No Pretraining + No CoT** *(Trained from scratch, minimal supervision)* | | | | | |
| HRM | ~27M | **40.3** | **5.0** | ‚ùå | ‚ùå | Proposed model; uses hierarchical recurrence, latent reasoning |
| Direct pred (8-layer Transformer) | ~27M | 15.8 | ~0.0 (implied) | ‚ùå | ‚ùå | Same size as HRM, but standard architecture fails on hard tasks |
| Liao & Gu (equivariant CNN) | ~10‚Äì50M (est.) | 15.8 | Not reported | ‚ùå | ‚ùå | Specialized architecture tailored to ARC; hand-designed inductive biases |
| | | | | | | |
| **Pretrained + CoT-Based** *(Large language models using step-by-step prompting)* | | | | | |
| o3-mini-high (GPT-4o variant) | ~48B | 34.5 | 1.3 | ‚úÖ | ‚úÖ | Top CoT model in evaluation; uses 128k context |
| Claude 3.7 8K | ~100‚Äì200B (est.) | 21.2 | 0.9 | ‚úÖ | ‚úÖ | Proprietary model; strong CoT capability |
| Deepseek R1 (est.) | ~100B+ | ~21.0 | ~0.0 | ‚úÖ | ‚úÖ | Estimated performance based on plot in paper |
| | | | | | | |
| **Pretrained + CoT (Other Notable Models)** | | | | | |
| AlphaGeometry 2 | ~100B+ | ~20‚Äì25 (est.) | Not tested | ‚úÖ | ‚úÖ | Specialized for geometric puzzles, not full ARC-AGI |
| GPT-4o (Base) | ~48B | ~30‚Äì35 (varies) | N/A | ‚úÖ | ‚úÖ | Public results vary; performance depends on prompt engineering |

---

### üîç Key Insights:

- **HRM is in a different league**: Despite using **no pretraining**, **no CoT**, and **1,000√ó fewer parameters**, it **outperforms all CoT-based models** on **ARC-AGI-1**.
- **Efficiency**: HRM achieves **40.3% on ARC-AGI-1** using only 960 training examples and 27M parameters ‚Äî a level of data and parameter efficiency unmatched by any other model.
- **CoT limitations**: Even the strongest CoT models struggle with **ARC-AGI-2**, which requires **compositional, multi-step abstraction** ‚Äî suggesting CoT has fundamental limits on novel reasoning.
- **Architecture > Scale**: HRM proves that **better internal reasoning design** can beat **scaling alone** ‚Äî a shift from ‚Äúbigger is better‚Äù to ‚Äúdeeper, structured computation wins.‚Äù

This hierarchical view underscores HRM‚Äôs significance: **it achieves superior reasoning not by being larger or more data-hungry, but by thinking differently ‚Äî deeply, internally, and adaptively.**