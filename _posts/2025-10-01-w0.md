---
title: "Writing-zero Implementation using Prime Intellect stack"
date: 2025-10-01
---


# Introduction

[writing-zero](https://www.alphaxiv.org/abs/2506.00103)

# Data Synthesis

As part of their cold-start SFT, they utilized high quality dataset where the size of the dataset is doubled by changing the order at which two responses are passed within the prompt. I have created a similar dataset using different LLMs and the [LitBench](https://huggingface.co/datasets/SAA-Lab/LitBench-Train) dataset.

Each prompt is repeated twice (once rejected first, once accepted response first), if the LLM is able to choose the correct response each time then we keep that reasoning trace.

## Self-Critique principle generation (SCP)

This concept first introduced in ----, it is a prompting technique asking the LLM to come up with a rubric and score each response against the rubric. The prompt looks similar to this:

```
.....
### 1. Develop Specific Evaluation Criteria for Literary Creativity
Based on the literary nature of the content and focusing specifically on creativity as the primary evaluation metric, refer to the **General Evaluation Criteria** to develop **Specific Evaluation Criteria** tailored to the current question. 
The specific criteria should supplement or adjust for the particular need and complement the general criteria. 
The specific criteria should include: 
- Dimension descriptions 
- A 1-10 scoring scale

### 3. Comparative Evaluation Using both the **General Evaluation Criteria** and the **Specific Evaluation Criteria**, conduct a comparative analysis of the two assistants’ responses for each dimension.

### 4. Weight Allocation Based on the general and specific evaluation criteria, allocate weights for each specific item. Ensure the total weight sums to 100%. Note: If any response scores low on the harmfulness criterion, increase the weight of harmfulness.

### 5. Scoring Method Score each evaluation dimension separately on a scale of 1 to 10, where 1 means completely unsatisfactory and 10 means fully satisfactory. After scoring, calculate the weighted average score for each response based on the weights of each dimension, resulting in a comprehensive score between 1 and 10 for each response.
————
.....
```

# Training

The prime-rl stack breaks the process into these components:
1. orchestrator
2. train
3. inference

And separate .toml files need to be passed for each in cmd:

```bash
uv run rl \
  --trainer @ examples/writing_zero/GenRM/rl/train.toml \
  --orchestrator @ examples/writing_zero/GenRM/rl/orch.toml \
  --inference @ examples/writing_zero/GenRM/rl/infer.toml \
  --model.name ... \
  --wandb.project ... \
  --wandb.name ...
```

## GenRM

### Cold-start fine tunning

[LitBench](https://app.primeintellect.ai/dashboard/environments/dmnsh001/litbench)

### GRPO

## Bootstrapped Relative Policy Optimization (BRPO)


[`w0-brpo`](https://app.primeintellect.ai/dashboard/environments/dmnsh001/w0-brpo)

# Evaluation results

Minimum training yields the same results as the [paper](https://www.alphaxiv.org/abs/2506.00103) suggested wherein BRPO on creative tasks does increase models ability on selecting better when given multiple choice questions (RewardBench2). This is evident by 2% increase (4.18% of original score) in [RewardBench](https://app.primeintellect.ai/dashboard/environments/primeintellect/reward-bench) and 0.148 increase within [WritingBench](https://app.primeintellect.ai/dashboard/environments/primeintellect/writing-bench)

| Benchmark\Model      | PrimeIntellect/Qwen3-4B | dmnsh/Qwen3-4b-W0-BRPO | Change (%) |
|------------------:|:-----------------------:|:----------------------:|:----------:|
| RewardBench2       | 0.478 | 0.498  | +4.18%    |
|  Writing Bench  | 6.864 | 7.012  | +2.16%    |


# Further improvement

More diverse synthetic data


Optimized training